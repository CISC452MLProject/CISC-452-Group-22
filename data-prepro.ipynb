{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code used to generate the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import shutil\n",
    "# from moviepy.video.io.VideoFileClip import VideoFileClip\n",
    "# from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('videos'):\n",
    "    shutil.unpack_archive('videos.zip', 'videos')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse metadata\n",
    "with open(\"data_unprocessed/metadata.txt\") as file:\n",
    "    lines =  np.array([line.rstrip().split(\";\") for line in file])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "newpath = r'./temp' \n",
    "if not os.path.exists(newpath):\n",
    "    os.makedirs(newpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally we would use a python library to download the youtube videos here. Unfortunatly a recent change to Youtubes cypher system has rendered all of these tools non-functional. Since our data set has a small number of long videos, we have opted to download them manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently the raw videos are not included in the repo, later I will include them in a zip archive if the github file limit allows it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.config import FFMPEG_BINARY\n",
    "import subprocess\n",
    "\n",
    "def ffmpeg_extract_subclip(\n",
    "    inputfile, start_time, end_time, outputfile=None, logger=\"bar\"\n",
    "):\n",
    "    \"\"\"Makes a new video file playing video file between two times.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    inputfile : str\n",
    "      Path to the file from which the subclip will be extracted.\n",
    "\n",
    "    start_time : float\n",
    "      Moment of the input clip that marks the start of the produced subclip.\n",
    "\n",
    "    end_time : float\n",
    "      Moment of the input clip that marks the end of the produced subclip.\n",
    "\n",
    "    outputfile : str, optional\n",
    "      Path to the output file. Defaults to\n",
    "      ``<inputfile_name>SUB<start_time>_<end_time><ext>``.\n",
    "    \"\"\"\n",
    "    if not outputfile:\n",
    "        name, ext = os.path.splitext(inputfile)\n",
    "        t1, t2 = [int(1000 * t) for t in [start_time, end_time]]\n",
    "        outputfile = \"%sSUB%d_%d%s\" % (name, t1, t2, ext)\n",
    "\n",
    "    cmd = [\n",
    "        FFMPEG_BINARY,\n",
    "        \"-y\",\n",
    "        \"-ss\",\n",
    "        \"%0.2f\" % start_time,\n",
    "        \"-i\",\n",
    "        inputfile,\n",
    "        \"-to\",\n",
    "        \"%0.2f\" % end_time,\n",
    "        \"-map\",\n",
    "        \"0\",\n",
    "        \"-vcodec\",\n",
    "        \"copy\",\n",
    "        \"-acodec\",\n",
    "        \"copy\",\n",
    "        \"-copyts\",\n",
    "        outputfile,\n",
    "    ]\n",
    "    subprocess.run(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = np.unique(lines.transpose()[4])\n",
    "\n",
    "i=0\n",
    "while i < len(lines):\n",
    "    \n",
    "    if float(lines[i][5]) == float(lines[i+1][5])  and lines[i][4] == lines[i+1][4]:\n",
    "        sample = lines[i if lines[i][6]> lines[i+1][6]  else i+1]\n",
    "        start = float(lines[i][5])\n",
    "        end = max(float(lines[i][6]), float(lines[i+1][6]))\n",
    "        \n",
    "        name = lines[i][0]+\"&\"+lines[i+1][0]\n",
    "        i+=2\n",
    "    else:\n",
    "        sample = lines[i]\n",
    "        start = float(sample[5])\n",
    "        end = float(sample[6])\n",
    "        name = sample[0]\n",
    "        i+=1\n",
    "\n",
    "    idx = np.where(links == sample[4])\n",
    "    video = sample[4].split(\"=\")[-1]\n",
    "    ## If its the first video, remove once all have been downloaded\n",
    "    # if idx[0][0] == 0:\n",
    "    ffmpeg_extract_subclip(\"videos/\"+video+\".mp4\", start, end, \"cropped/\" + name+\".mp4\")\n",
    "        \n",
    "       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Create a zip archive of the output so the videos can be commited to github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\mikeG\\\\Documents\\\\school\\\\cisc-452\\\\CISC-452-Group-22\\\\videos_processed.zip'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.path.isfile(\"videos_processed.zip\"):\n",
    "    os.remove(\"videos_processed.zip\")\n",
    "shutil.make_archive(\"videos_processed\", 'zip', \"cropped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cropped\\1_2018-07-12_1.mp4\n",
    "\n",
    "run = \"34_2018-07-06_2\"\n",
    "\n",
    "import cv2\n",
    "vidcap = cv2.VideoCapture('cropped\\\\'+run+'.mp4')\n",
    "success,image = vidcap.read()\n",
    "count = 0\n",
    "\n",
    "with open(\"data_unprocessed/skeletons/\"+run+\".data\") as file:\n",
    "    data =  np.array([line.rstrip().split(\"#\") for line in file])\n",
    "\n",
    "frame_count =int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT)) - 1\n",
    "\n",
    "print(data)\n",
    "\n",
    "offset = frame_count - int(data[-1][0])\n",
    "\n",
    "print (\"Number of frames: \", int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT)) - 1)\n",
    "while success:\n",
    "    if count < offset:\n",
    "        count += 1\n",
    "        success,image = vidcap.read()\n",
    "        continue\n",
    "    cv2.imwrite(\"img/frame%d.jpg\" % (count-offset), image)     # save frame as JPEG file      \n",
    "    success,image = vidcap.read()\n",
    "    print('Read a new frame: ', success)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def print_to_log(*args, sep=' ', end='\\n', file=sys.stdout, flush=False):\n",
    "    \"\"\"\n",
    "    Custom print function that appends the output to 'log.log' and writes to the console.\n",
    "\n",
    "    Args:\n",
    "        *args: Values to be printed.\n",
    "        sep (str): Separator between values (default: ' ').\n",
    "        end (str): End character (default: '\\n').\n",
    "        file: File-like object to write to (default: sys.stdout).\n",
    "        flush (bool): Whether to forcibly flush the stream.\n",
    "    \"\"\"\n",
    "    message = sep.join(map(str, args)) + end  # Construct the message\n",
    "\n",
    "    # Append to the log file\n",
    "    with open(\"log.log\", \"a\") as log_file:\n",
    "        log_file.write(message)\n",
    "\n",
    "\n",
    "def parse_metadata(metadata_path):\n",
    "    metadata = {}\n",
    "    with open(metadata_path, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(';')\n",
    "            metadata[parts[0]] = {\n",
    "                'id_climber': parts[1],\n",
    "                'date': parts[2],\n",
    "                'run_number': parts[3],\n",
    "                'url': parts[4],\n",
    "                'start': float(parts[5]),\n",
    "                'end': float(parts[6]),\n",
    "                'time_sec': float(parts[7]),\n",
    "                'time_frames': int(parts[8]),\n",
    "                'finished': int(parts[9]),\n",
    "                'side': parts[10],\n",
    "                'fps': float(parts[11])\n",
    "            }\n",
    "    return metadata\n",
    "\n",
    "def parse_skeletons(skeletons_dir):\n",
    "    skeleton_data = {}\n",
    "    for file_name in os.listdir(skeletons_dir):\n",
    "        run_id = file_name.replace('.data', '')\n",
    "        skeleton_data[run_id] = []\n",
    "        with open(os.path.join(skeletons_dir, file_name), 'r') as f:\n",
    "            for line in f:\n",
    "                if \"NULL\" in line:\n",
    "                    continue\n",
    "                parts = line.strip().split('#')\n",
    "                frame_number = int(parts[0])\n",
    "                keypoints = parts[1].split(';')\n",
    "                keypoints = [float(coord) for kp in keypoints for coord in kp.split(',')]\n",
    "                skeleton_data[run_id].append({\n",
    "                    'frame_number': frame_number,\n",
    "                    'keypoints': keypoints\n",
    "                })\n",
    "    return skeleton_data\n",
    "\n",
    "def generate_coco_json(images_dir, skeletons, metadata, output_json):\n",
    "    coco_data = {\n",
    "        'images': [],\n",
    "        'annotations': [],\n",
    "        'categories': [{\n",
    "            'id': 1,\n",
    "            'name': 'person',\n",
    "            'keypoints': [f'j{i+1}' for i in range(16)],\n",
    "            'skeleton': [[i, i+1] for i in range(1, 16)]  # Simple linear skeleton structure\n",
    "        }]\n",
    "    }\n",
    "\n",
    "    annotation_id = 1\n",
    "    for run_id, frames in skeletons.items():\n",
    "        climber_metadata = metadata.get(run_id, {})\n",
    "        for frame in frames:\n",
    "            image_id = len(coco_data['images']) + 1\n",
    "            frame_number = frame['frame_number']\n",
    "            image_path = os.path.join(images_dir, f'frame{frame_number}.png')\n",
    "            coco_data['images'].append({\n",
    "                'id': image_id,\n",
    "                'file_name': image_path,\n",
    "                'height': 1920,\n",
    "                'width': 1080\n",
    "            })\n",
    "            coco_data['annotations'].append({\n",
    "                'id': annotation_id,\n",
    "                'image_id': image_id,\n",
    "                'category_id': 1,\n",
    "                'keypoints': frame['keypoints'],\n",
    "                'num_keypoints': sum(1 for x in frame['keypoints'][2::3] if x > 0),\n",
    "                'climber_id': climber_metadata.get('id_climber', 'unknown'),\n",
    "                'run_metadata': climber_metadata \n",
    "            })\n",
    "            annotation_id += 1\n",
    "\n",
    "    with open(output_json, 'w') as f:\n",
    "        json.dump(coco_data, f, indent=4)\n",
    "\n",
    "# Paths\n",
    "images_dir = 'img'\n",
    "skeletons_dir = 'data_unprocessed/skeletons'\n",
    "metadata_file = 'data_unprocessed/metadata.txt'\n",
    "output_json = 'climbData.json'\n",
    "test_json = 'test.json'\n",
    "\n",
    "# Process\n",
    "metadata = parse_metadata(metadata_file)\n",
    "skeletons = parse_skeletons(skeletons_dir)\n",
    "\n",
    "generate_coco_json(images_dir, skeletons, metadata, output_json)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CISC-452-Group-22",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
